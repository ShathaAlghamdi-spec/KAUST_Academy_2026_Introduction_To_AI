{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1ub0yd9vh3e"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAHJJaavh3g"
      },
      "source": [
        "# Pytorch Basics\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esiI8WoVvh3g"
      },
      "source": [
        "# **ğŸ“Œ How Data is Represented in Deep Learning?**\n",
        "\n",
        "Deep learning models process data in the form of **tensors** (multi-dimensional arrays).  \n",
        "The shape of the tensor depends on the type of data being used.\n",
        "\n",
        "## **ğŸ”¹ 1ï¸âƒ£ Tabular Data (Structured Data)**\n",
        "- **Shape:** `(batch_size, features)`\n",
        "- Each **row** is a sample, and each **column** is a feature.\n",
        "- **Handled by:** `nn.Linear` (Fully Connected Layers).\n",
        "\n",
        "## **ğŸ”¹ 2ï¸âƒ£ Image Data (Computer Vision)**\n",
        "- **Shape:** `(batch_size, channels, height, width)`\n",
        "  - **RGB Image:** `channels = 3` (Red, Green, Blue).\n",
        "  - **Grayscale Image:** `channels = 1` (sometimes omitted).\n",
        "- **Handled by:** `nn.Conv2d` (Convolutional Layers).\n",
        "\n",
        "| **Data Type** | **Tensor Shape** | **Handled by** |\n",
        "|--------------|-----------------|---------------|\n",
        "| **Tabular Data** | `(batch_size, features)` | `nn.Linear` |\n",
        "| **Image Data** | `(batch_size, channels, height, width)` | `nn.Conv2d` |\n",
        "\n",
        "âœ… Each data type has a specific tensor representation and requires different processing techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNmsF7Tbvh3h"
      },
      "source": [
        "# **ğŸ“Œ How to Change the Dimensions in PyTorch?**\n",
        "\n",
        "Manipulating tensor shapes is essential in deep learning. PyTorch provides several functions to modify tensor dimensions.\n",
        "\n",
        "## **ğŸ”¹ 1ï¸âƒ£ Flatten**\n",
        "- Converts **any shape** to `(batch_size, features)`.\n",
        "- **Example:**  \n",
        "  `(batch_size, channels, height, width) â†’ (batch_size, features)`\n",
        "\n",
        "## **ğŸ”¹ 2ï¸âƒ£ Squeeze**\n",
        "- **Removes dimensions** with size `1`.\n",
        "- **Example:**  \n",
        "  `(1, 32, 3, 28, 28) â†’ (32, 3, 28, 28)`\n",
        "\n",
        "## **ğŸ”¹ 3ï¸âƒ£ Unsqueeze**\n",
        "- **Adds a dimension** with size `1` at a specified position.\n",
        "- **Example:**  \n",
        "  `(3, 28, 28) â†’ (1, 3, 28, 28)`\n",
        "\n",
        "## **ğŸ”¹ 4ï¸âƒ£ Permute**\n",
        "- **Reorders the dimensions** of a tensor by specifying the **new order of indices**.\n",
        "- **Example:**  \n",
        "  `(32, 28, 28, 3) â†’ permute(0, 3, 1, 2) â†’ (32, 3, 28, 28)`\n",
        "\n",
        "## **ğŸ”¹ 5ï¸âƒ£ View (works similar to reshape)**\n",
        "- **Reshapes a tensor freely** while maintaining the same number of elements.\n",
        "- **Example:**  \n",
        "  `(32, 28, 28, 3) â†’ view(-1, 28*28*3) â†’ `(32, 28*28*3)`\n",
        "\n",
        "| **Operation** | **Function** | **Purpose** | **Example Transformation** |\n",
        "|--------------|-------------|-------------|----------------------------|\n",
        "| **Flatten** | `.flatten()` | Convert tensor to (batch, features) | `(32, 3, 28, 28) â†’ (32, 3*28*28)` |\n",
        "| **Squeeze** | `.squeeze()` | Remove dims of size 1 | `(1, 3, 28, 28) â†’ (3, 28, 28)` |\n",
        "| **Unsqueeze** | `.unsqueeze(dim)` | Add a dim of size 1 | `(3, 28, 28) â†’ (1, 3, 28, 28)` |\n",
        "| **Permute** | `.permute(dims)` | Change order of dimensions | `(32, 28, 28, 3) â†’ (32, 3, 28, 28)` |\n",
        "| **View** | `.view(shape)` | Reshape freely | `(32, 28, 28, 3) â†’ (32, 28*28*3)` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:26.524571Z",
          "iopub.status.busy": "2025-02-01T13:52:26.524183Z",
          "iopub.status.idle": "2025-02-01T13:52:30.375828Z",
          "shell.execute_reply": "2025-02-01T13:52:30.374699Z",
          "shell.execute_reply.started": "2025-02-01T13:52:26.524542Z"
        },
        "trusted": true,
        "id": "_GVG7tCLvh3h",
        "outputId": "2fe76986-0280-4001-b198-a8120088667d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flatten: torch.Size([32, 2352])\n",
            "Squeeze: torch.Size([3, 28, 28])\n",
            "Unsqueeze: torch.Size([1, 3, 28, 28])\n",
            "Permute: torch.Size([32, 3, 28, 28])\n",
            "View: torch.Size([32, 2352])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1ï¸âƒ£ Flatten - Convert any shape to (batch_size, features) \"2D\"\n",
        "x = torch.randn(32, 3, 28, 28)\n",
        "x_flat = x.flatten(start_dim=1)\n",
        "print(\"Flatten:\", x_flat.shape)  # (32, 2352) , by multiply the last 3\n",
        "\n",
        "# 2ï¸âƒ£ Squeeze - Remove dimensions with size 1\n",
        "x = torch.randn(1, 3, 28, 28)\n",
        "x_sq = x.squeeze()\n",
        "print(\"Squeeze:\", x_sq.shape)  # (3, 28, 28)\n",
        "\n",
        "# 3ï¸âƒ£ Unsqueeze - Add a new dimension of size 1\n",
        "x = torch.randn(3, 28, 28)\n",
        "x_unsq = x.unsqueeze(0)\n",
        "print(\"Unsqueeze:\", x_unsq.shape)  # (1, 3, 28, 28)\n",
        "\n",
        "# 4ï¸âƒ£ Permute - Reorder dimensions\n",
        "x = torch.randn(32, 28, 28, 3)  # (batch, height, width, channels) , \"the order is false\"\n",
        "x_perm = x.permute(0, 3, 1, 2)  # (batch, channels, height, width)\n",
        "print(\"Permute:\", x_perm.shape)  # (32, 3, 28, 28)\n",
        "\n",
        "# 5ï¸âƒ£ View - Reshape freely while keeping same number of elements\n",
        "x = torch.randn(32, 28, 28, 3)\n",
        "x_view = x.view(32, -1)  # Flatten all except batch , -1 means you gonna calculate it\n",
        "print(\"View:\", x_view.shape)  # (32, 28*28*3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTcdP0Qvh3i"
      },
      "source": [
        "### ğŸ”¹ Changing Data Type or Moving Data/Model to CPU/GPU  \n",
        "\n",
        "PyTorch allows you to **change the datatype** of a tensor and **move it between CPU and GPU** using `.to()`.  \n",
        "\n",
        "---\n",
        "\n",
        "### âœ… **Change Datatype**\n",
        "Use `.to(dtype)` to convert a tensor's data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.377553Z",
          "iopub.status.busy": "2025-02-01T13:52:30.377089Z",
          "iopub.status.idle": "2025-02-01T13:52:30.397024Z",
          "shell.execute_reply": "2025-02-01T13:52:30.395751Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.377523Z"
        },
        "trusted": true,
        "id": "eCHhKp_tvh3i",
        "outputId": "cbd64b96-7c93-4b7c-9827-992f477e31a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.float16\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a float32 tensor , by default create a 32\n",
        "x = torch.tensor([1.2, 2.3, 3.4], dtype=torch.float32)\n",
        "print(x.dtype)  # Output: torch.float32\n",
        "\n",
        "# Convert to float16\n",
        "x_half = x.to(torch.float16)\n",
        "print(x_half.dtype)  # Output: torch.float16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCeXvBrevh3j"
      },
      "source": [
        "### âœ… **Move Tensors to GPU (if available)**\n",
        "Use `.to(device)` to move a tensor to GPU for faster computation.\n",
        "\n",
        "**GPUs are faster and more efficient** in most cases when training or inferencing deep learning models.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.399777Z",
          "iopub.status.busy": "2025-02-01T13:52:30.399396Z",
          "iopub.status.idle": "2025-02-01T13:52:30.410675Z",
          "shell.execute_reply": "2025-02-01T13:52:30.409468Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.399747Z"
        },
        "trusted": true,
        "id": "Whdb4Zw0vh3j",
        "outputId": "7c53d30c-9c02-4774-e03e-cc6370cd5da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Automatically select CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a tensor and move it to GPU\n",
        "x_gpu = x.to(device) # GPUÙ„ÙƒÙ† Ø¹Ù„Ù‰ Ø§Ù„ Tensor x Ù†Ø³Ø®Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù†\n",
        "print(x_gpu.device)  # Output: cuda:0 (if GPU is available) or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psNPBSsPvh3j"
      },
      "source": [
        "Note: When training a model, always move BOTH the model and data to the same device. Otherwise, you will get an error like this:\n",
        "\n",
        "`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_CcFSpMvh3j"
      },
      "source": [
        "### ğŸ§  AI Layers\n",
        "\n",
        "PyTorch provides various **neural network layers** to build deep learning models. Below are some of the most commonly used layers.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ 1ï¸âƒ£ Linear Layer (`nn.Linear`)\n",
        "\n",
        "### ğŸ“Œ **Usage**\n",
        "1. Used for **fully connected layers** (Dense layers).\n",
        "2. Typically used as the **final layer** in CNNs for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.412554Z",
          "iopub.status.busy": "2025-02-01T13:52:30.412185Z",
          "iopub.status.idle": "2025-02-01T13:52:30.463528Z",
          "shell.execute_reply": "2025-02-01T13:52:30.462470Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.412518Z"
        },
        "trusted": true,
        "id": "7PuwrRF5vh3j",
        "outputId": "85c93452-69e1-43bc-d758-6fcf128a9fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: torch.Size([16, 5])\n",
            "Output Shape: torch.Size([16, 3])\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a Linear layer , Ù„ÙƒÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (5 features) Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ…\n",
        "linear_layer = nn.Linear(in_features=5, out_features=3) # Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ Ø³ØªØ®Ø±Ø¬Ù‡Ø§ Ø§Ù„Ø·Ø¨Ù‚Ø© Ù„ÙƒÙ„ Ø¹ÙŠÙ†Ø© 3\n",
        "\n",
        "# Random input tensor (batch_size=16 Ø¹ÙŠÙ†Ø§Øª, in_features=5 Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ù„ÙƒÙ„ Ø¹ÙŠÙ†Ø©) Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¯Ø®Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
        "x = torch.randn(16, 5)\n",
        "\n",
        "# Forward pass , Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØªØ­Ø±Ùƒ Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ÙˆØªØ¨ÙˆØª\n",
        "output = linear_layer(x)\n",
        "\"\"\"\n",
        "(Ø£ÙŠ ÙƒÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù€ 16) x ÙƒÙ„ ØµÙ ÙÙŠ\n",
        "biasØ«Ù… ÙŠØ¶Ø§Ù Ø§Ù„Ù€ W^T ÙŠØªÙ… Ø¶Ø±Ø¨Ù‡ ÙÙŠ\n",
        "output Ù‡Ø°Ø§ ÙŠÙ†ØªØ¬ ØµÙ Ø¬Ø¯ÙŠØ¯ ÙÙŠ\n",
        "\n",
        "output = xâ‹… W^T + b\n",
        "\"\"\"\n",
        "\n",
        "print(\"Input Shape:\", x.shape)       # (16, 5) , (batch_size, in_features) ØªØ¹Ø·ÙŠ x.shape\n",
        "print(\"Output Shape:\", output.shape)  # (16, 3) , Ù†ÙØ³Ù‡Ø§ Ø¨Ø³ Ù„Ù„Ø§ÙˆØª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF8KsmDyvh3j"
      },
      "source": [
        "## ğŸ”¹ 2ï¸âƒ£ Convolutional Layer (`nn.Conv2d`)\n",
        "\n",
        "\n",
        "###  ğŸ“Œ **Usage**\n",
        "âœ… `nn.Conv2d` is used for **feature extraction** in images.  \n",
        "ğŸš« It **does not perform classification or regression**â€”you need a `nn.Linear` layer for that.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.465080Z",
          "iopub.status.busy": "2025-02-01T13:52:30.464719Z",
          "iopub.status.idle": "2025-02-01T13:52:30.547569Z",
          "shell.execute_reply": "2025-02-01T13:52:30.546561Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.465047Z"
        },
        "trusted": true,
        "id": "gRGWkkXpvh3j",
        "outputId": "a5443ddb-35f2-46ab-93db-868e05329695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: torch.Size([16, 3, 32, 32])\n",
            "Output Shape: torch.Size([16, 16, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a Conv2D layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "\n",
        "# Random input tensor (batch_size=16, channels=3, height=32, width=32)\n",
        "x = torch.randn(16, 3, 32, 32)\n",
        "\n",
        "# Forward pass , ÙŠÙ†Ø²Ù„Ù‚ Ø¹Ù„Ù‰ ÙƒÙ„ ØµÙˆØ±Ø© ÙˆÙŠØ­Ø³Ø¨ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¶Ø±Ø¨ ÙˆØ§Ù„Ø¬Ù…Ø¹ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙ„ØªØ±(3x3)ÙƒÙ„ ÙÙ„ØªØ±\n",
        "output = conv_layer(x)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)       # (16, 3, 32, 32)\n",
        "print(\"Output Shape:\", output.shape)  # (16, 16, 30, 30)\n",
        "\n",
        "\"\"\"\n",
        "padding ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø°Ø§ Ù…Ø§ÙÙŠ\n",
        "\n",
        "Output_Size= (Input Sizeâˆ’Kernel Sizeâ€‹) / Stride +1\n",
        "\n",
        "Input = 32 Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©\n",
        "Kernel = 3\n",
        "Stride = 1 (default)\n",
        "Padding = 0 (default)\n",
        "\n",
        "ØµØ§Ø±Øª 30 Ø¨Ø¯Ù„ 32width , heightÙ‡Ø°Ø§ Ø§Ù„Ø³Ø¨Ø¨ Ù„ÙŠØ´ Ø§Ù„Ù€\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø§Ù„ÙÙ„Ø§ØªØ± Ø²Ø§Ø¯Øª Ù…Ù† 3 ÙÙŠ Ø§Ù„Ø§Ù†Ø¨ÙˆØª Ø§Ù„Ù‰ 16 ÙØ§Ù„Ø§ÙˆØªØ¨ÙˆØª\n",
        "# ÙŠÙ‚Ù„ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ Ù„Ù„ØµÙˆØ±Ø© padding Ø£Ùˆ Ù‚Ù„ kernel ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø¹Ø¯Ø¯\n",
        "\n",
        "# kernel = (Ù„Ø£Ù†Ù‡ ÙŠØºØ·ÙŠ Ù…Ø³Ø§Ø­Ø© Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©)\n",
        "# padding = (Ù„Ø£Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³Ø§Ø­Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØµÙˆØ±Ø©)"
      ],
      "metadata": {
        "id": "omdMb8HJKqx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpFkz6c0vh3k"
      },
      "source": [
        "# **ğŸ“Œ Important Note**\n",
        "When you use **Convolutional layers (`nn.Conv2d`)**, the **output channels tend to be larger** than the input channels (unlike `nn.Linear`).  \n",
        "\n",
        "### **Why?**\n",
        "- Each convolutional layer **extracts more useful features** from the input.\n",
        "- As more filters are applied, the **number of channels increases**.\n",
        "- Meanwhile, **spatial size (height & width) decreases**.\n",
        "\n",
        "âœ… **This allows the model to capture richer features while reducing unnecessary spatial details.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30wumGMvh3k"
      },
      "source": [
        "The **output image size** after a convolutional layer is calculated using the formula (or you can get it by trial-and-error):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEzRvgTBvh3k"
      },
      "source": [
        "![image.png](https://i.imgur.com/8XKBFBU.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoYBDoT8vh3k"
      },
      "source": [
        "## ğŸ”¹ 3ï¸âƒ£ Pooling (`nn.MaxPool2d` / `nn.AvgPool2d`)**\n",
        "\n",
        "###  ğŸ“Œ **Usage**\n",
        "- **Reduces spatial dimensions** (height & width) while retaining important features.\n",
        "- Typically used **after Conv2D layers** to downsample feature maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.548925Z",
          "iopub.status.busy": "2025-02-01T13:52:30.548526Z",
          "iopub.status.idle": "2025-02-01T13:52:30.569245Z",
          "shell.execute_reply": "2025-02-01T13:52:30.568050Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.548885Z"
        },
        "trusted": true,
        "id": "Tx8sJRJkvh3k",
        "outputId": "86b91a62-1e7b-40e7-9a6a-44b2550b270a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Conv2D: torch.Size([2, 16, 32, 32])\n",
            "After ReLU: torch.Size([2, 16, 32, 32])\n",
            "After Pooling: torch.Size([2, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define layers\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "relu = nn.ReLU()    ## An old friend ;)     Do you remember why do we need it here?\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# Sample input tensor (batch_size=2, channels=3, height=32, width=32)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "\n",
        "# ğŸ”¹ Step 1: Convolution Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø²Ø§ÙŠØ§\n",
        "x = conv(x)\n",
        "print(\"After Conv2D:\", x.shape)  # (2, 16, 32, 32) , height and width DOESN'T change because padding = 1\n",
        "\n",
        "# ğŸ”¹ Step 2: ReLU Activation ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ù„Ø¨Ø© Ø§Ù„Ù‰ ØµÙØ±\n",
        "x = relu(x)\n",
        "print(\"After ReLU:\", x.shape)  # (2, 16, 32, 32) ÙŠØ¨Ù‚Ù‰ Ù…Ø«Ù„ Ù…Ø§Ù‡Ùˆ\n",
        "\n",
        "# ğŸ”¹ Step 3: Pooling (Reduces spatial size) ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù‡Ù…Ø©\n",
        "x = pool(x)\n",
        "print(\"After Pooling:\", x.shape)  # (2, 16, 16, 16) Ù†Ù‚ØµÙˆØ§ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ ÙˆØ§Ù„Ø¹Ø±Ø¶ Ù„Ù„Ù†ØµÙ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPY0EWkrvh3k"
      },
      "source": [
        "### **But wait a minute, if Conv2D extracts the features only, how should we do the classification/regression? ğŸ¤”**\n",
        "\n",
        "Conv2D layers are responsible for extracting features (edges, textures, patterns, etc.) from the input data, but they **do not perform classification or regression directly**.  \n",
        "\n",
        "To classify or predict, we need to **map the extracted features** to the desired output using **fully connected layers (`nn.Linear`)** after flattening the feature maps.\n",
        "\n",
        "---\n",
        "\n",
        "### **ğŸ—ï¸ CNN Structure Example**\n",
        "A typical CNN model for classification or regression follows this pattern:\n",
        "\n",
        "1ï¸âƒ£ **`nn.Conv2d`** â†’ Extracts features .  \n",
        "2ï¸âƒ£ **`nn.MaxPool2d`** â†’ Reduces feature map size to focus on important information.  \n",
        "3ï¸âƒ£ **`nn.Conv2d`** â†’ Extracts more features.  \n",
        "\n",
        "... Add more layers if you want\n",
        "\n",
        "4ï¸âƒ£ **Flatten & `nn.Linear`** â†’ Maps extracted features to output classes or predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D Ø¨Ø¹Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ù†Ø­ØªØ§Ø¬ Ù†Ø­ÙˆÙ„Ù‡Ø§ Ø§Ù„Ù‰\n",
        "# nn.linear Ø¹Ø´Ø§Ù† ØªØ¯Ø®Ù„ Ø¹Ù„Ù‰ flatten Ø¨ÙˆØ§Ø³Ø·Ø©"
      ],
      "metadata": {
        "id": "ufw6ohH7Qz7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OX3aBgBvh3k"
      },
      "source": [
        "![image.png](https://i.imgur.com/fwNdXJs.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8q6q16Yvh3k"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **ğŸ”¹ Example: Classification with CNN**\n",
        "| **Layer**                  | **Purpose**                        | **Example Shape Transformation** |\n",
        "|----------------------------|-------------------------------------|-----------------------------------|\n",
        "| **Input Image**            | Raw input                          | `(batch_size, channels, height, width)` |\n",
        "| **`nn.Conv2d`**            | Extract features                   | `(32, 3, 32, 32) â†’ (32, 16, 30, 30)` |\n",
        "| **`nn.MaxPool2d`**         | Downsample feature maps            | `(32, 16, 30, 30) â†’ (32, 16, 15, 15)` |\n",
        "| **`nn.Conv2d`**            | Extract more features              | `(32, 16, 15, 15) â†’ (32, 32, 13, 13)` |\n",
        "| **Flatten**                | Prepare for fully connected layers | `(32, 32, 13, 13) â†’ (32, 32*13*13)` |\n",
        "| **`nn.Linear`**            | Map features to classes/predictions| `(32, 32*13*13) â†’ (32, 10)` (for 10 classes) |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.571030Z",
          "iopub.status.busy": "2025-02-01T13:52:30.570501Z",
          "iopub.status.idle": "2025-02-01T13:52:30.631073Z",
          "shell.execute_reply": "2025-02-01T13:52:30.629954Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.570988Z"
        },
        "trusted": true,
        "id": "Y1D4IHf4vh3k",
        "outputId": "f56e9759-e199-4107-e0f5-f626df94f5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Conv2D: torch.Size([2, 16, 30, 30])\n",
            "After ReLU: torch.Size([2, 16, 30, 30])\n",
            "After Pooling: torch.Size([2, 16, 15, 15])\n",
            "After Flatten: torch.Size([2, 3600])\n",
            "After Linear (Logits): torch.Size([2, 10])\n",
            "After Softmax (Probabilities): tensor([0.0916, 0.0663, 0.1165, 0.1155, 0.0827, 0.0699, 0.1514, 0.0984, 0.0919,\n",
            "        0.1159], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define layers\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)  # Conv2D layer\n",
        "relu = nn.ReLU()  # Activation function\n",
        "pool = nn.MaxPool2d(kernel_size=2)  # Pooling layer\n",
        "flatten = nn.Flatten()  # Flatten layer to prepare for Linear\n",
        "linear = nn.Linear(16 * 15 * 15, 10)  # Fully connected layer for classification (10 classes)\n",
        "softmax = nn.Softmax(dim=1)  # ## Another old friend ;) Ø§Ù„Ø­Ø³Ø§Ø¨ ÙŠØªÙ… Ø¹Ø¨Ø± Ø§Ù„Ø§Ø¹Ù…Ø¯Ø©\n",
        "\n",
        "# Sample input tensor (batch_size=2, channels=3, height=32, width=32)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "\n",
        "# ğŸ”¹ Step 1: Convolution\n",
        "x = conv(x)\n",
        "print(\"After Conv2D:\", x.shape)  # (2, 16, 30, 30)\n",
        "\n",
        "# ğŸ”¹ Step 2: ReLU Activation\n",
        "x = relu(x)\n",
        "print(\"After ReLU:\", x.shape)  # (2, 16, 30, 30)\n",
        "\n",
        "# ğŸ”¹ Step 3: Pooling (Reduces spatial size)\n",
        "x = pool(x)\n",
        "print(\"After Pooling:\", x.shape)  # (2, 16, 15, 15)\n",
        "\n",
        "# ğŸ”¹ Step 4: Flatten (Convert to batch_size, features)\n",
        "x = flatten(x)\n",
        "print(\"After Flatten:\", x.shape)  # (2, 16*15*15)\n",
        "\n",
        "# ğŸ”¹ Step 5: Fully Connected Layer\n",
        "x = linear(x)\n",
        "print(\"After Linear (Logits):\", x.shape)  # (2, 10) â†’ 10 values, one per class , Ù…Ù† ÙÙˆÙ‚ Ù…ÙƒØªÙˆØ¨Ù‡ Ø§Ù„10\n",
        "\n",
        "# ğŸ”¹ Step 6: Softmax (Convert logits to probabilities)\n",
        "x = softmax(x)\n",
        "print(\"After Softmax (Probabilities):\", x[0])  # Probabilities for each class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZk07BWHvh3k"
      },
      "source": [
        "# **Great! Now you know how to build a CNN model!**\n",
        "\n",
        "However, PyTorch has a specific structure to organize your workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ“Œ PyTorch Workflow Organization**\n",
        "\n",
        "### **It consists of 4 main components:**\n",
        "1ï¸âƒ£ **Dataset Class**  \n",
        "- Handles loading and preprocessing data.  \n",
        "- Converts raw data (e.g., images, CSVs) into model-ready tensors.  \n",
        "\n",
        "2ï¸âƒ£ **Model Class**  \n",
        "- Defines the architecture of your neural network (e.g., layers, activations).  \n",
        "\n",
        "3ï¸âƒ£ **Training Loop**  \n",
        "- Updates model weights using backpropagation and optimizers.  \n",
        "- Computes the loss for every batch and adjusts the parameters to minimize it.  \n",
        "\n",
        "4ï¸âƒ£ **Validation Loop**  \n",
        "- Evaluates the model's performance on a validation set.  \n",
        "- Does not update weights but computes metrics like accuracy or loss.  \n",
        "\n",
        "---\n",
        "\n",
        "### **ğŸ“Œ Note:**\n",
        "All the labs will follow this structure. You will just modify the content for different tasks, such as changing datasets, architectures, or loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OsRHaFlvh3k"
      },
      "source": [
        "# **ğŸ“Œ Dataset Class**\n",
        "\n",
        "- The **Dataset Class** is designed to **load and preprocess only one sample** at a time.\n",
        "- The **DataLoader** uses the Dataset Class to load **multiple samples (batches)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4d113lvh3k"
      },
      "source": [
        "## **1ï¸âƒ£ It Could Be Ready-to-Use:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Yetd1dM9vh3k"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "train_dataset = MNIST(root=\"./data\", train=True, download=True) # Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… ÙÙŠÙ‡ ØªØ®Ø²ÙŠÙ† Ø£Ùˆ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª root=\"./data\"\n",
        "test_dataset = MNIST(root=\"./data\", train=False, download=True) # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© download\n",
        "\n",
        "# OR Ù†ÙØ³ Ø§Ù„Ø´ÙŠ Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª Ø§Ø®Ø±Ù‰ Ø­Ù‚Øª ØµÙˆØ± Ù…Ù„ÙˆÙ†Ø©\n",
        "\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "train_dataset = CIFAR10(root=\"./data\", train=True, download=True)\n",
        "test_dataset = CIFAR10(root=\"./data\", train=False, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77DkNsAYvh3k"
      },
      "source": [
        "## **2ï¸âƒ£ Or You Have to Define It Yourself (We will explore how to define it in another lab).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v08JHZ9vh3k"
      },
      "source": [
        "# **ğŸ“Œ Model Class**\n",
        "---\n",
        "\n",
        "## **ğŸ“Œ Key Components:**\n",
        "1ï¸âƒ£ **Define Layers (`__init__` method):**  \n",
        "- Use PyTorch modules (e.g., `nn.Conv2d`, `nn.Linear`) to create the model's architecture.  \n",
        "\n",
        "2ï¸âƒ£ **Forward Pass (`forward` method):**  \n",
        "- Specify how the input should be fed through the layers step by step.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1/ Ù„ØªØ­Ø¯ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ­ØªØ§Ø¬Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "# 2/ ØªØ­Ø¯Ø¯ ÙƒÙŠÙÙŠØ© Ù…Ø±ÙˆØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø¹Ø¨Ø± Ø§Ù„Ø·Ø¨Ù‚Ø§Øª"
      ],
      "metadata": {
        "id": "ehAq8zB_wDWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alIRyppbvh3k"
      },
      "source": [
        "## **1ï¸âƒ£ You may define It Yourself:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:45.676080Z",
          "iopub.status.busy": "2025-02-01T13:52:45.675543Z",
          "iopub.status.idle": "2025-02-01T13:52:45.683583Z",
          "shell.execute_reply": "2025-02-01T13:52:45.682535Z",
          "shell.execute_reply.started": "2025-02-01T13:52:45.676041Z"
        },
        "trusted": true,
        "id": "KMWMqZ5wvh3k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        1ï¸âƒ£ Define all layers in the model.\n",
        "        \"\"\"\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        # Convolutional Layer + Activation + Pooling\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected Layer , flatten Ø¨Ø¹Ø¯Ù…Ø§ Ø³ÙˆÙŠÙ†Ø§\n",
        "        self.fc = nn.Linear(16 * 16 * 16, 10)  # Output 10 classes\n",
        "\n",
        "        # Softmax Layer\n",
        "        self.softmax = nn.Softmax(dim=1)  # Apply along the class dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        2ï¸âƒ£ Define the forward pass (how data flows through the model).\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)  # Convolution\n",
        "        x = self.relu(x)  # Activation\n",
        "        x = self.pool(x)  # Pooling\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten for FC layer\n",
        "        x = self.fc(x)  # Fully connected layer\n",
        "        x = self.softmax(x)  # Convert logits to probabilities\n",
        "        return x  # Returns probability distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlSxnaSYvh3l"
      },
      "source": [
        "###\n",
        "## **2ï¸âƒ£ Or It Could Be Given Ready-to-Use (Will explore in another lab):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "gto2CKAEvh3l"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "#(pretrained) Ù‡Ø°Ø§ Ø³ÙŠÙˆÙØ± Ø§Ù„ÙˆÙ‚Øª ÙˆÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ù‘Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ , ResNet Ù…Ù…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù…Ø«Ù„\n",
        "\n",
        "import torchvision.models as models\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision.models : Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©  \n",
        "\n",
        "resnet18 : ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 18 Ø·Ø¨Ù‚Ø© ResNet  Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù…Ù† Ø¹Ø§Ø¦Ù„Ø©\n",
        "\n",
        "pretrained=True: ImageNet ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¹Ù„Ù‰\n",
        "\n",
        "Ø£Ùˆ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø¨Ù†ÙØ³Ùƒ Ø¥Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² forward Ù„Ø§ ØªØ­ØªØ§Ø¬ Ù„ØªØ¹Ø±ÙŠÙ"
      ],
      "metadata": {
        "id": "UI7BTicQyE6H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wFBhvDCvh3l"
      },
      "source": [
        "# **ğŸ“Œ Training Loop**\n",
        "\n",
        "## **What is the Training Loop?**\n",
        "The **training loop** is responsible for **updating the model's weights** so that it learns to minimize the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.279176Z",
          "iopub.status.busy": "2025-02-01T13:52:46.278877Z",
          "iopub.status.idle": "2025-02-01T13:52:46.284659Z",
          "shell.execute_reply": "2025-02-01T13:52:46.283750Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.279148Z"
        },
        "trusted": true,
        "id": "psE5gTQHvh3l"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode, you will understand why later , Ù„ÙˆØ¨ ØªØ­Ø¯ÙŠØ« Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙˆØ¯Ù„ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in dataloader: # Ù†Ø¬ÙŠØ¨ ÙƒÙ„ Ø¨Ø§ØªØ´\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients ØªØµÙÙŠØ± Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
        "        loss.backward()  # Backpropagation (compute gradients) Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„ÙƒÙ„ ÙˆØ²Ù†\n",
        "        optimizer.step()  # Update model parameters ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§ÙˆØ²Ø§Ù†\n",
        "\n",
        "        # Collect the loss , batch Ø¬Ù…Ø¹ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„ÙƒÙ„\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)  # Return average loss , epoch Ø¥Ø±Ø¬Ø§Ø¹ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„ÙƒÙ„\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1a9Gm9gvh3l"
      },
      "source": [
        "# **ğŸ“Œ Validation Loop**\n",
        "\n",
        "## **What is the Validation Loop?**\n",
        "- The **validation loop** is used to **evaluate model performance** on unseen data.  \n",
        "- Unlike the training loop, **it does NOT update the modelâ€™s weights**.  \n",
        "- It helps track **loss and accuracy** to monitor model improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.286262Z",
          "iopub.status.busy": "2025-02-01T13:52:46.285804Z",
          "iopub.status.idle": "2025-02-01T13:52:46.304261Z",
          "shell.execute_reply": "2025-02-01T13:52:46.303139Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.286150Z"
        },
        "trusted": true,
        "id": "wl1ixB8Ovh3l"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode, you will understand why later\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation ØªØ¹Ø·ÙŠÙ„ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)  # Forward pass Ø¹Ù„Ù‰ Ø¨Ø§ØªØ´ Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # (Optional) Compute accuracy\n",
        "            predictions = outputs.argmax(dim=1)  # Get class with highest probability , Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ Ø°Ùˆ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù„ÙƒÙ„ Ø¹ÙŠÙ†Ø©\n",
        "            correct += (predictions == labels).sum().item() # Ø¹Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©\n",
        "            total += labels.size(0) # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ÙƒÙ„ÙŠ\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader) # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø±Ø©\n",
        "    accuracy = 100 * correct / total  # Compute accuracy in percentage\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88_w66ycvh3l"
      },
      "source": [
        "# **ğŸ“Œ Full Training Process in PyTorch**\n",
        "\n",
        "Now that you understand the **Dataset Class, Model Class, Training Loop, and Validation Loop**, it's time to put everything together into a **full training process**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.305727Z",
          "iopub.status.busy": "2025-02-01T13:52:46.305338Z",
          "iopub.status.idle": "2025-02-01T13:54:13.696582Z",
          "shell.execute_reply": "2025-02-01T13:54:13.695558Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.305698Z"
        },
        "trusted": true,
        "id": "3gIeoZzNvh3l",
        "outputId": "df48e298-a858-4804-ac9e-a8d9defa6fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Validation Accuracy = 95.12%\n",
            "Epoch 2: Validation Accuracy = 96.73%\n",
            "Epoch 3: Validation Accuracy = 97.17%\n",
            "Epoch 4: Validation Accuracy = 97.51%\n",
            "Epoch 5: Validation Accuracy = 97.66%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# ğŸ”¹ Load MNIST Dataset , Ø¨Ø§Ù„Ø§Ø¨ÙŠØ¶ ÙˆØ§Ù„Ø§Ø³ÙˆØ¯ Ùˆ 10 ÙƒÙ„Ø§Ø³Ø²MNIST Ù‡ÙŠ ØµÙˆØ± 28Ã—28\n",
        "\n",
        "transform = transforms.ToTensor() #  Ø¨ÙŠÙ† 0 Ùˆ 1 Tensors ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰\n",
        "train_dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        1ï¸âƒ£ Define all layers in the model.\n",
        "        \"\"\"\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        # Convolutional Layer + Activation + Pooling\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1) # Ù‚Ù†Ø§Ø© ÙˆØ­Ø¯Ø© ÙŠØ¹Ù†ÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ø¨ÙŠØ¶ Ùˆ Ø§Ø³ÙˆØ¯\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(16 * 14 * 14, 10)  # Output 10 classes Ù‚Ù„Ù„Ù‡Ø§ Ù„Ù„Ù†ØµÙ\n",
        "\n",
        "        # Softmax Layer\n",
        "        self.softmax = nn.Softmax(dim=1)  # Apply along the class dimension Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        2ï¸âƒ£ Define the forward pass (how data flows through the model).\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)  # Convolution\n",
        "        x = self.relu(x)  # Activation\n",
        "        x = self.pool(x)  # Pooling\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten for FC layer\n",
        "        x = self.fc(x)  # Fully connected layer\n",
        "        x = self.softmax(x)  # Convert logits to probabilities\n",
        "        return x  # Returns probability distribution\n",
        "\n",
        "\n",
        "\n",
        "# Training and Validation Loops , Dropout Ùˆ BatchNorm ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØ³Ù…Ø­ Ø¨ØªÙØ¹ÙŠÙ„\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device) # CPU , GPU Ø§Ø±Ø³Ø§Ù„ Ø¨Ø§ØªØ´Ø² Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù‰\n",
        "        optimizer.zero_grad() # ØªØµÙÙŠØ± Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©\n",
        "        loss = criterion(model(images), labels) # compute LOSS\n",
        "        loss.backward() # Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„ÙƒÙ„ ÙˆØ²Ù†\n",
        "        optimizer.step() # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad(): # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device) # Ù†Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "            outputs = model(images) # Forward pass ÙÙ‚Ø·\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item() # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø§Ø¹Ù„Ù‰\n",
        "            total += labels.size(0) # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù ÙƒÙ„ Ø¨Ø§ØªØ´\n",
        "    return 100 * correct / total  # Return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run Training\n",
        "model = CustomModel() # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # multi class ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© , Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Ø§Ø¯Ù… Ù„ØªØ­Ø¯ÙŠØ« Ø§ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙˆØ¯Ù„ , lr = learning rate\n",
        "\n",
        "for epoch in range(5):  # Train for 5 epochs , ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯Ù„ 5 Ù…Ø±Ø§Øª Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    train_one_epoch(model, train_loader, criterion, optimizer, device) # ØªØ¯Ø±ÙŠØ¨\n",
        "    accuracy = validate(model, test_loader, criterion, device) # ØªÙ‚ÙŠÙŠÙ…\n",
        "    print(f\"Epoch {epoch+1}: Validation Accuracy = {accuracy:.2f}%\") # print results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQEYpfWovh3o"
      },
      "source": [
        "![image.png](https://i.imgur.com/1xbDOQX.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cjb7CvAvh3o"
      },
      "source": [
        "### Contributed by: Mohamed Eltayeb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4C7vDCbvh3o"
      },
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}